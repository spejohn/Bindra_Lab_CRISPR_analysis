# CRISPR Analysis Pipeline Benchmarking

This module provides tools to benchmark the CRISPR analysis pipeline against published results from labs that used the same raw data.

## Directory Structure

The benchmarking module expects a specific directory structure:

1. **Generated Results**: Results from our pipeline (`crispr_analysis_pipeline_results/`)
   ```
   crispr_analysis_pipeline_results/
   ├── experiment1/
   │   ├── contrast1/
   │   │   ├── contrast1_gDZ.csv
   │   │   └── contrast1_gMGK.csv
   │   └── contrast2/
   │       ├── contrast2_gDZ.csv
   │       └── contrast2_gMGK.csv
   └── experiment2/
       └── contrast1/
           ├── contrast1_gDZ.csv
           └── contrast1_gMGK.csv
   ```

2. **Published Results**: Results from published papers (`published_crispr_results/`)
   ```
   published_crispr_results/
   ├── experiment1/
   │   ├── contrast1_pDZ.csv
   │   ├── contrast1_pMGK.csv
   │   ├── contrast2_pDZ.csv
   │   └── contrast2_pMGK.csv
   └── experiment2/
       ├── contrast1_pDZ.csv
       └── contrast1_pMGK.csv
   ```

Note that the published results should have the same file names as the generated results, but with `_pDZ.csv` or `_pMGK.csv` extensions instead of `_gDZ.csv` or `_gMGK.csv`.

## Usage

Run the benchmarking script:

```bash
python -m analysis_pipeline.benchmarking.benchmark_pipeline \
    --generated-dir /path/to/crispr_analysis_pipeline_results \
    --published-dir /path/to/published_crispr_results \
    --output-dir /path/to/benchmarking/output
```

### Command-line Arguments

- `--generated-dir`: Directory containing results generated by our pipeline
- `--published-dir`: Directory containing published results to compare against
- `--output-dir`: Directory to store benchmarking results
- `--experiment`: (Optional) Specific experiment to benchmark
- `--contrast`: (Optional) Specific contrast to benchmark
- `--no-plots`: (Optional) Disable generation of correlation plots (plots are generated by default)
- `--output-file`: (Optional) Name of the output metrics file (default: "crispr_screen_benchmark_metrics.csv")
- `--log-level`: (Optional) Set the logging level (default: "INFO")

## Output

The benchmarking script will:

1. Find matching file pairs between the generated and published results
2. Copy the files to a QC directory structure for analysis
3. Run direct comparisons and generate metrics
4. Create correlation plots (unless disabled with `--no-plots`)
5. Output a CSV file with detailed QC metrics

### Output Structure

```
/path/to/benchmarking/output/
├── crispr_screen_benchmark_metrics.csv  # QC metrics
├── direct_comparison/                   # Direct comparison results
│   ├── direct_comparison_metrics.csv
│   └── plots/                          # Correlation plots
│       ├── experiment1/
│       │   └── contrast1/
│       │       ├── normz_correlation_DZ.png
│       │       ├── fdr_synth_correlation_DZ.png
│       │       └── ...
│       └── experiment2/
│           └── ...
└── qc_data/                            # Copied files for QC
    ├── experiment1/
    │   └── contrast1/
    │       ├── contrast1_gDZ.csv
    │       ├── contrast1_pDZ.csv
    │       ├── contrast1_gMGK.csv
    │       └── contrast1_pMGK.csv
    └── experiment2/
        └── ...
```

## Metrics

The benchmarking script calculates various metrics to evaluate how well our pipeline reproduces published results:

### DrugZ Metrics

- R² and slope for:
  - normZ values
  - FDR values
  - Rank values
  - Number of observations

### MAGeCK Metrics

- R² and slope for:
  - LFC values
  - Scores
  - FDR values
  - Rank values
  - Number of guides 